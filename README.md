# 🚀 My Data Science Playground  
*A self-taught 20-week program to refresh and master data science skills*

## 📌 Program Goals
By the end of this program, I aim to:
- ✅ **Fine-tune large language models** for my domain
- ✅ **Build retrieval-augmented generation (RAG) pipelines** (e.g., chatbots on manuals)
- ✅ **Engineer predictive-maintenance solutions** for manufacturing equipment

## 📆 Curriculum Overview

### Weeks 0–2: Prerequisites & Setup
**📌 Objectives**: Learn core tools & foundational math  
- 🛠 **Python & Jupyter** → Write Python scripts, use pandas/numpy  
- 🔗 **Git & GitHub Basics** → Track & push projects to GitHub  
- 🧠 **Linear Algebra & Stats** → Vectors, matrices, distributions, hypothesis testing  
- ⚙️ **Environment Setup** → Install Anaconda, set up DS/ML environment  
**📌 Assignment**: Basic pandas/numpy exercises & sensor data analysis

### Weeks 3–6: Machine Learning Foundations
**📌 Objectives**: Grasp core ML algorithms & model evaluation  
- 🔢 **Regression Models** → Linear Regression, Ridge, Decision Trees  
- 🔄 **Classification Models** → Logistic Regression, Random Forest, SVM  
- 🔎 **Unsupervised Learning** → K-means clustering, PCA  
- 📊 **Model Evaluation** → Cross-validation, hyperparameter tuning  
**📌 Hands-on Lab**: Predict machine failure & optimize models

### Weeks 7–10: Deep Learning & NLP Basics
**📌 Objectives**: Understand neural networks & transformers  
- 🤖 **Neural Networks** → Build simple feed-forward models  
- 🔥 **PyTorch Basics** → Train models using PyTorch  
- 📝 **Text Processing** → Tokenization, embeddings with Hugging Face  
- ⚡ **Transformers** → Fine-tune BERT/GPT models for text tasks  
**📌 Project**: Sentiment analysis classifier using fine-tuned transformer

### Weeks 11–13: Fine-Tuning Large Language Models (LLMs)
**📌 Objectives**: Adapt LLMs for domain-specific tasks  
- 🔄 **Fine-Tuning vs LoRA** → Trade-offs in compute & data requirements  
- 📁 **Data Preparation** → Formatting JSONL for training LLMs  
- 🏗️ **Training & Evaluation** → Fine-tune model on maintenance logs  
- 📈 **Experiment Tracking** → Visualize performance with Weights & Biases  
**📌 Assignment**: Fine-tune small model on categorized maintenance tickets

### Weeks 14–16: Retrieval-Augmented Generation (RAG)
**📌 Objectives**: Build pipelines that retrieve knowledge for LLMs  
- 🔍 **Information Retrieval** → Vector embeddings & similarity metrics  
- 📂 **Vector Databases** → FAISS, Pinecone for scalable search  
- 🔗 **RAG Architectures** → Combine retriever & generator  
- 🚀 **Deployment & Testing** → Serve answers using FastAPI  
**📌 Project**: Chatbot that answers questions from engineering manuals

### Weeks 17–20: Predictive Maintenance
**📌 Objectives**: Process time-series sensor data to predict failures  
- 📊 **Feature Engineering** → Extract insights using tsfresh  
- ⚠️ **Anomaly Detection** → Detect outliers with Isolation Forest, Autoencoders  
- ⏳ **RUL Prediction** → Predict Remaining Useful Life using NASA datasets  
- 📡 **Monitoring & Dashboards** → Visualize sensor data using Streamlit/Grafana  
**📌 Assignment**: Predict engine failure time using NASA’s C-MAPSS dataset

## 🎓 Capstone Integration
**Final Project**: Bringing everything together  
- 🔄 Fine-tune LLM on maintenance logs  
- 🔎 Build RAG system for documentation retrieval  
- ⚠️ Develop predictive maintenance models for proactive alerts  
- 📂 Integrate unified pipeline where engineers can:  
  - Query issues  
  - Get data-grounded answers  
  - Monitor machine health

## 📂 Repository Structure
📁 My-ds-playground
├── 📂 Beyond_Week_1 # Week-specific projects & datasets
├── 📂 models # Trained ML/DL models
├── 📂 data # Raw and processed datasets
├── 📂 notebooks # Jupyter notebooks with exercises
├── 📂 scripts # Python scripts for automation
└── README.md # This file (learning plan overview)

## 🚀 How I Track Progress
- 📌 Update this README after each module  
- 📌 Push weekly code commits  
- 📌 Use GitHub Issues & Projects to track milestones

**📅 Start Date**: May 2, 2025  
**🎯 Target Completion**: September 19, 2025
